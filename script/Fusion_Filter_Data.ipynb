{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "577c6317",
   "metadata": {},
   "source": [
    "# TCGA Fusion & Preprocessing Notebook ðŸ§¬\n",
    "\n",
    "### Overview ðŸŒŸ\n",
    "This notebook is designed to preprocess and integrate multiple datasets, including:\n",
    "\n",
    "- **Expression matrices** ðŸ§ª\n",
    "- **Clinical data** ðŸ¥\n",
    "- **Sample metadata** ðŸ—‚ï¸\n",
    "\n",
    "The goal is to merge these datasets into a unified format suitable for training a **Deep Learning Model (MLP)**. ðŸš€\n",
    "\n",
    "### Why are we doing this? ðŸ¤”\n",
    "\n",
    "1. **Deep Learning Models**, such as Multi-Layer Perceptrons (MLPs), require structured and clean input data. This ensures the model can learn meaningful patterns without being hindered by inconsistencies or missing values.\n",
    "2. By combining clinical, expression, and sample data, we create a **comprehensive dataset** that captures both molecular and clinical features. This enables the model to:\n",
    "   - Predict outcomes more accurately. ðŸŽ¯\n",
    "   - Identify key biomarkers. ðŸ”¬\n",
    "   - Support personalized medicine approaches. ðŸ’Š\n",
    "\n",
    "### What will this notebook achieve? âœ…\n",
    "- Preprocess raw data files to ensure consistency.\n",
    "- Map and align identifiers across datasets.\n",
    "- Merge all relevant features into a single dataset ready for **MLP training**.\n",
    "\n",
    "Letâ€™s get started! ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc46c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Librairies and Paths\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Paths (edit these)\n",
    "\n",
    "EXPR_PATH = \"C:\\\\Users\\\\assou\\\\Documents\\\\PYTHON\\\\BIP12\\\\gdc_download_20251125_142547.268493\"\n",
    "CLIN_PATH = \"../data/clinical.tsv\"\n",
    "SAMPLES_PATH = \"../data/sample.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab5ba30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Loading\n",
    "\n",
    "clinical = pd.read_csv(CLIN_PATH, sep=\"\\t\")\n",
    "samples = pd.read_csv(SAMPLES_PATH, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7e4024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate expression data since each file represents ~ one sample \n",
    "# takes around 1 minute to run\n",
    "\n",
    "expression = []\n",
    "\n",
    "for root, dirs, files in os.walk(EXPR_PATH):\n",
    "    for f in files:\n",
    "        if f.endswith(\".tsv\"):\n",
    "            path = os.path.join(root, f) # full path to the file\n",
    "\n",
    "            try:\n",
    "                df = pd.read_csv(path, sep=\"\\t\", comment=\"#\")\n",
    "                sample_id = f.split(\".\")[0]   # sample name based on filename\n",
    "\n",
    "                # Add only the expression column for this sample\n",
    "                expression.append(\n",
    "                    df.set_index(\"gene_id\")[[\"fpkm_uq_unstranded\"]]\n",
    "                    .rename(columns={\"fpkm_uq_unstranded\": f})\n",
    "                )\n",
    "\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "# Concatenate all expression columns horizontally\n",
    "expr_matrix = pd.concat(expression, axis=1)\n",
    "expr_matrix = expr_matrix[4:].T.copy()\n",
    "expr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed98f33",
   "metadata": {},
   "source": [
    "## Downloading the Expression Matrix ðŸ’¾\n",
    "\n",
    "You can download the **expression matrix** as it is using the following code snippet. However, it is **highly recommended** to preprocess the data first to ensure better quality for downstream analysis. ðŸ§¹\n",
    "\n",
    "### Why preprocess the data? ðŸ¤”\n",
    "- **Filtering**: Remove irrelevant or noisy data.\n",
    "- **Normalization**: Ensure consistency across samples.\n",
    "- **Feature selection**: Focus on the most informative genes.\n",
    "\n",
    "Preprocessing will help improve the performance and reliability of your machine learning models. ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f98e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#expr_matrix.to_csv(\"../data/TCGA_expression_matrix.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b34ad93",
   "metadata": {},
   "source": [
    "## Mapping File Names to Entity Submitter IDs ðŸ—‚ï¸\n",
    "\n",
    "In this section, we will:\n",
    "\n",
    "- ðŸ”— Create a **mapping dataframe** using the metadata file provided by TCGA.\n",
    "- ðŸ—ƒï¸ Map **`file names`** to **`entity submitter IDs`**.\n",
    "- ðŸ“‹ Include other relevant IDs required for merging with clinical data later on.\n",
    "\n",
    "### Why is this important? ðŸ¤”\n",
    "This mapping is a **crucial step** in preparing the data for:\n",
    "- Accurate integration of clinical and expression datasets.\n",
    "- Ensuring consistency across all data sources for downstream analysis. ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5923e322",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapping from metadata case_id to case submitter_id\n",
    "with open(\"../data/metadata.cart.2025-11-25.json\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "rows = []\n",
    "\n",
    "for entry in meta:\n",
    "    file_name = entry[\"file_name\"]\n",
    "    file_id = entry[\"file_id\"]\n",
    "    \n",
    "    ent = entry[\"associated_entities\"][0]\n",
    "    \n",
    "    aliquot_id = ent[\"entity_id\"]\n",
    "    case_id = ent[\"case_id\"]\n",
    "    submitter_id = ent[\"entity_submitter_id\"].split(\"-\")[0:3]\n",
    "    submitter_id = \"-\".join(submitter_id)\n",
    "    \n",
    "    rows.append({\n",
    "        \"file_name\": file_name,\n",
    "        \"aliquot_id\": aliquot_id,\n",
    "        \"case_id\": case_id,\n",
    "        \"submitter_id\": submitter_id\n",
    "    })\n",
    "\n",
    "map_df = pd.DataFrame(rows)\n",
    "\n",
    "#Use this command to download the .csv file of the mapping dataframe (dictionnary)\n",
    "#map_df.to_csv(\"metadata_mapping.csv\", index=False)\n",
    "\n",
    "map_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bdd80c",
   "metadata": {},
   "source": [
    "## Mapping File Names to Submitter IDs in the Expression Matrix ðŸ—‚ï¸\n",
    "\n",
    "This section focuses on:\n",
    "\n",
    "- ðŸ”— Mapping **`file names`** to **`submitter IDs`** within the expression matrix.\n",
    "- âœ… Ensuring that the expression data is correctly aligned with the clinical and sample metadata.\n",
    "\n",
    "This mapping is a critical step for facilitating accurate downstream analysis and integration. ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244dc25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sub_id(filename):\n",
    "    match = map_df[map_df[\"file_name\"] == filename]\n",
    "    if len(match) > 0:\n",
    "        return match[\"submitter_id\"].values[0]\n",
    "    return filename  # fallback to filename if not found\n",
    "\n",
    "expr_matrix.index = expr_matrix.index.map(get_sub_id)\n",
    "expr_matrix.index.name = \"submitter_id\"\n",
    "\n",
    "expr_matrix.to_csv(\"../expression_matrix_sub.tsv\", sep=\"\\t\", index=True)\n",
    "expr_matrix\n",
    "\n",
    "#You should now have an expression matrix with submitter_ids as row indices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ddd345",
   "metadata": {},
   "source": [
    "# Adding Clinical and Sample Labels ðŸ·ï¸\n",
    "\n",
    "## Second Mapping: Transforming `case_id` to `submitter_id` in Clinical Data\n",
    "\n",
    "In this step, we will use the mapping dataframe (`map_df`) to:\n",
    "\n",
    "- Replace **`case_id`** values in the clinical data with their corresponding **`submitter_id`** values from the metadata.\n",
    "\n",
    "This transformation ensures consistency between the clinical data and the metadata, which is essential for accurate data integration and analysis. ðŸ”„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa833768",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Colonnes disponibles dans clinical :\", clinical.columns.tolist())\n",
    "#set case_id as index\n",
    "clinical = clinical.set_index(\"cases.case_id\")\n",
    "#Map des case_id en submitter_id dans clinical\n",
    "def get_sub_id_case(filename):\n",
    "    match = map_df[map_df[\"case_id\"] == filename]\n",
    "    if len(match) > 0:\n",
    "        return match[\"submitter_id\"].values[0]\n",
    "    return filename  # fallback to filename if not found\n",
    "\n",
    "clinical.index = clinical.index.map(get_sub_id_case)\n",
    "clinical.index.name = \"submitter_id\"\n",
    "\n",
    "clinical.to_csv(\"../clinical_sub.tsv\", sep=\"\\t\", index=True)\n",
    "clinical\n",
    "\n",
    "#You should now have clinical data with submitter_ids as row indices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b31bfd5",
   "metadata": {},
   "source": [
    "## Sample Data ðŸ§ª\n",
    "\n",
    "This step is crucial for ensuring that all datasets are aligned and can be effectively merged for comprehensive analysis. âœ…\n",
    "\n",
    "### Key Notes:\n",
    "- In the **sample data**, the `submitter_id` is already present. ðŸŽ¯\n",
    "- Therefore, no additional mapping is required. ðŸš«\n",
    "- The goal here is simply to set the `submitter_id` as the index for easier access and consistency. ðŸ“‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965f1e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set les cases.submitter_id comme index\n",
    "samples = samples.set_index(\"cases.submitter_id\")\n",
    "samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64d0b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's recheck the clinical data to ensure all case_ids have been mapped correctly.\n",
    "clinical.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeb2a62",
   "metadata": {},
   "source": [
    "# Merging Clinical, Expression, and Sample Data ðŸ”—\n",
    "\n",
    "In this step, we will:\n",
    "\n",
    "- ðŸ§¬ Combine **clinical data**, **expression data**, and **sample data** into a unified dataset.\n",
    "- ðŸ“Š Ensure all datasets are properly aligned using `submitter_id` as the common key.\n",
    "\n",
    "This fusion is essential for creating a comprehensive dataset ready for downstream analysis and machine learning tasks. ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f385d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = expr_matrix \\\n",
    "    .merge(clinical[[\"cases.primary_site\", \"demographic.age_at_index\",\"diagnoses.ajcc_pathologic_stage\",\n",
    "                     \"diagnoses.ajcc_pathologic_t\",\"diagnoses.classification_of_tumor\",\"diagnoses.primary_diagnosis\",\n",
    "                     \"treatments.treatment_or_therapy\", \"treatments.treatment_type\"]], \n",
    "                     left_index=True, right_index=True, how=\"inner\") \\\n",
    "    .merge(samples[[\"samples.sample_type\",\"samples.tissue_type\"]], left_index=True, right_index=True, how=\"left\")\n",
    "\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159e57ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#changer les gene_ids en gene_names\n",
    "gene_map = pd.read_csv(\"../data/gene_name.tsv\", sep=\"\\t\")\n",
    "gene_map.head()\n",
    "gene_dict = pd.Series(gene_map.gene_name.values, index=gene_map.gene_id).to_dict()\n",
    "final = final.rename(columns=gene_dict)\n",
    "\n",
    "final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485c64cd",
   "metadata": {},
   "source": [
    "# Download the Final Dataset ðŸ’¾\n",
    "\n",
    "### File Name: `final_TCGA_thyroid_data.csv`\n",
    "\n",
    "âš ï¸ **Important Notes:**\n",
    "- The final dataset can be **very large** depending on the number of samples and features included. ðŸ“Š\n",
    "- It is recommended to apply **filtering** before downloading if you only need specific subsets of the data. âœ‚ï¸\n",
    "\n",
    "The filtering section follows, and you can adapt it based on the type of analysis you want to perform. ðŸ”\n",
    "I also recommend saving the final dataset in a compressed format (e.g., `.zip` or `.gz`) to reduce file size and facilitate easier sharing and storage. ðŸ“¦\n",
    "\n",
    "To view the dataset, I use the extension \"Data Wrangler\" in VSCode, which allows for easy exploration of large CSV files, and gives you a summary of each column. ðŸ“‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf964c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering\n",
    "# For example, filter for thyroid cancer samples only\n",
    "#final = final[final[\"diagnoses.classification_of_tumor\"] == \"Thyroid Carcinoma\"]\n",
    "\n",
    "#You can filter based on the treatment type :\n",
    "#final = final[final[\"treatments.treatment_type\"] == \"Chemotherapy\"]\n",
    "\n",
    "#Filtering based on the sample type :\n",
    "#final = final[final[\"samples.sample_type\"] == \"Primary Tumor\"]\n",
    "\n",
    "#Filtering based on the stage :\n",
    "#final = final[final[\"diagnoses.ajcc_pathologic_stage\"] == \"Stage I\"]\n",
    "\n",
    "#filtering based on age : \n",
    "#final = final[final[\"demographic.age_at_index\"] >= 50]\n",
    "\n",
    "#final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a423951d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final.to_csv(\"../data/final_TCGA_thyroid_data.csv\", sep=\"\\t\", index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioinfo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
